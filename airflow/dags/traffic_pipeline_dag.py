from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import time

default_args = {
    'owner': 'noura',
    'start_date': datetime(2026, 1, 5),
    'retries': 2,
    'retry_delay': timedelta(minutes=2),
    'execution_timeout': timedelta(minutes=30)
}

dag = DAG(
    'traffic_pipeline_big_data',
    default_args=default_args,
    schedule_interval='@hourly',
    catchup=False,
    description='Pipeline Big Data pour analyse du trafic urbain'
)

def wait_for_data():
    """Attendre que des donnÃ©es soient disponibles"""
    print("â³ Attente de gÃ©nÃ©ration de donnÃ©es (60 secondes)...")
    time.sleep(60)
    print("âœ… DonnÃ©es gÃ©nÃ©rÃ©es")

# TÃ¢che 1 : VÃ©rifier que les services sont prÃªts
check_services = BashOperator(
    task_id='check_services',
    bash_command='''
    echo "ğŸ” VÃ©rification des services..."
    docker ps | grep kafka-producer || exit 1
    docker ps | grep kafka-consumer || exit 1
    docker ps | grep spark || exit 1
    echo "âœ… Tous les services sont actifs"
    ''',
    dag=dag
)

# TÃ¢che 2 : Attendre la gÃ©nÃ©ration de donnÃ©es
wait_data = PythonOperator(
    task_id='wait_for_data_generation',
    python_callable=wait_for_data,
    dag=dag
)

# TÃ¢che 3 : VÃ©rifier les donnÃ©es dans HDFS
check_hdfs_data = BashOperator(
    task_id='check_hdfs_data',
    bash_command='''
    echo "ğŸ“Š VÃ©rification des donnÃ©es dans HDFS..."
    docker exec namenode hdfs dfs -ls /data/raw/traffic/ || exit 1
    COUNT=$(docker exec namenode hdfs dfs -ls -R /data/raw/traffic/ | grep -c "traffic.json" || echo "0")
    echo "âœ… Fichiers trouvÃ©s : $COUNT"
    if [ "$COUNT" -eq "0" ]; then
        echo "âŒ Aucune donnÃ©e trouvÃ©e"
        exit 1
    fi
    ''',
    dag=dag
)

# TÃ¢che 4 : Traitement Spark
spark_processing = BashOperator(
    task_id='spark_processing',
    bash_command='''
    echo "ğŸ”¥ Lancement du job Spark..."
    docker exec spark spark-submit \
        --master local[*] \
        --driver-memory 2g \
        --executor-memory 2g \
        /app/spark_job.py
    
    if [ $? -eq 0 ]; then
        echo "âœ… Job Spark terminÃ© avec succÃ¨s"
    else
        echo "âŒ Ã‰chec du job Spark"
        exit 1
    fi
    ''',
    dag=dag
)

# TÃ¢che 5 : VÃ©rifier les rÃ©sultats analytics
validate_results = BashOperator(
    task_id='validate_analytics',
    bash_command='''
    echo "ğŸ” VÃ©rification des rÃ©sultats analytics..."
    docker exec namenode hdfs dfs -ls /data/analytics/traffic/by_zone || exit 1
    docker exec namenode hdfs dfs -ls /data/analytics/traffic/by_road_type || exit 1
    echo "âœ… RÃ©sultats analytics disponibles"
    ''',
    dag=dag
)

# TÃ¢che 6 : GÃ©nÃ©rer un rapport
generate_report = BashOperator(
    task_id='generate_report',
    bash_command='''
    echo "ğŸ“„ GÃ©nÃ©ration du rapport..."
    TIMESTAMP=$(date +"%Y-%m-%d %H:%M:%S")
    echo "==================================="
    echo "RAPPORT PIPELINE TRAFIC URBAIN"
    echo "Date: $TIMESTAMP"
    echo "==================================="
    
    echo "ğŸ“Š Statistiques HDFS:"
    docker exec namenode hdfs dfs -du -s -h /data/raw/traffic/
    docker exec namenode hdfs dfs -du -s -h /data/analytics/traffic/
    
    echo ""
    echo "âœ… Pipeline exÃ©cutÃ© avec succÃ¨s"
    ''',
    dag=dag
)

# DÃ©finir les dÃ©pendances
check_services >> wait_data >> check_hdfs_data >> spark_processing >> validate_results >> generate_report