from pyspark.sql import SparkSession
from pyspark.sql.functions import avg, count, col

spark = SparkSession.builder \
    .appName("TrafficProcessing") \
    .config("spark.hadoop.fs.defaultFS", "hdfs://namenode:9000") \
    .config("spark.sql.shuffle.partitions", "4") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

print("ğŸš€ Spark Job - Traitement des donnÃ©es de trafic")

try:
    # CHEMIN CORRIGÃ‰ avec la vraie structure
    input_path = "hdfs://namenode:9000/data/raw/traffic/date=*/hour=*/zone=*/traffic.json"
    print(f"ğŸ“– Lecture des donnÃ©es depuis: {input_path}")
    
    df = spark.read.json(input_path)
    row_count = df.count()
    print(f"ğŸ“Š Nombre total d'Ã©vÃ©nements: {row_count}")
    
    if row_count == 0:
        print("âš ï¸ Aucune donnÃ©e trouvÃ©e")
    else:
        print("\nğŸ“„ AperÃ§u des donnÃ©es:")
        df.show(5, truncate=False)
        
        # Statistiques par zone
        print("\nğŸ“ˆ Calcul des statistiques par zone...")
        traffic_stats = df.groupBy("zone").agg(
            count("*").alias("total_events"),
            avg("vehicle_count").alias("avg_vehicle_count"),
            avg("average_speed").alias("avg_speed"),
            avg("occupancy_rate").alias("avg_occupancy")
        ).orderBy(col("total_events").desc())
        
        print("\nâœ… Statistiques de trafic par zone:")
        traffic_stats.show(truncate=False)
        
        # Statistiques par type de route
        print("\nğŸ“ˆ Calcul des statistiques par type de route...")
        road_stats = df.groupBy("road_type").agg(
            count("*").alias("total_events"),
            avg("vehicle_count").alias("avg_vehicle_count"),
            avg("average_speed").alias("avg_speed")
        ).orderBy(col("total_events").desc())
        
        print("\nâœ… Statistiques par type de route:")
        road_stats.show(truncate=False)
        
        # Zones congestionnÃ©es
        print("\nğŸš¦ Zones congestionnÃ©es:")
        congested = df.groupBy("zone").agg(
            avg("vehicle_count").alias("avg_vehicles"),
            avg("average_speed").alias("avg_speed"),
            avg("occupancy_rate").alias("avg_occupancy")
        ).filter((col("avg_speed") < 40) & (col("avg_occupancy") > 60))
        congested.show(truncate=False)
        
        # Sauvegarder les rÃ©sultats
        print("\nğŸ’¾ Sauvegarde des rÃ©sultats...")
        traffic_stats.write.mode("overwrite").parquet("hdfs://namenode:9000/data/analytics/traffic/by_zone")
        road_stats.write.mode("overwrite").parquet("hdfs://namenode:9000/data/analytics/traffic/by_road_type")
        congested.write.mode("overwrite").parquet("hdfs://namenode:9000/data/analytics/traffic/congested_zones")
        
        print("\nâœ… Job Spark terminÃ© avec succÃ¨s!")

except Exception as e:
    print(f"\nâŒ Erreur: {e}")
    import traceback
    traceback.print_exc()
finally:
    spark.stop()
    print("ğŸ‘‹ SparkSession fermÃ©e")